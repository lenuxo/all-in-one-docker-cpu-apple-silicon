"""
æ‰¹é‡éŸ³é¢‘åˆ†æç«¯ç‚¹
æä¾›å¤šæ–‡ä»¶æ‰¹é‡å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒä»»åŠ¡ç®¡ç†å’Œè¯¦ç»†è¿›åº¦è·Ÿè¸ª
"""

import time
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime
from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks
import structlog

from ..models import (
    BatchAnalysisRequest,
    BatchAnalysisResponse,
    TaskStatus,
    ErrorResponse,
    ModelType
)
from ..services.analysis_service import AnalysisService
from ..utils import validate_audio_file, get_audio_duration
from ..utils.memory_file_handler import memory_file_handler

logger = structlog.get_logger()

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter()

# åˆå§‹åŒ–åˆ†ææœåŠ¡
analysis_service = AnalysisService()

# æ‰¹é‡ä»»åŠ¡å­˜å‚¨
batch_tasks: Dict[str, Dict[str, Any]] = {}

@router.post(
    "/analyze/batch",
    response_model=BatchAnalysisResponse,
    responses={
        200: {
            "description": "æ‰¹é‡åˆ†æä»»åŠ¡åˆ›å»ºæˆåŠŸ",
            "content": {
                "application/json": {
                    "example": {
                        "success": True,
                        "task_id": "batch_20241123_001",
                        "message": "æ‰¹é‡åˆ†æä»»åŠ¡å·²åˆ›å»ºï¼Œå…±5ä¸ªæ–‡ä»¶",
                        "estimated_time": "5-10åˆ†é’Ÿ",
                        "file_count": 5,
                        "priority": 1,
                        "files": [
                            {"filename": "song1.wav", "size": "15.2MB", "status": "valid"},
                            {"filename": "song2.mp3", "size": "8.7MB", "status": "valid"}
                        ]
                    }
                }
            }
        },
        400: {"description": "è¯·æ±‚å‚æ•°é”™è¯¯", "model": ErrorResponse},
        413: {"description": "æ–‡ä»¶è¿‡å¤§", "model": ErrorResponse},
        422: {"description": "æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ", "model": ErrorResponse}
    },
    summary="æ‰¹é‡åˆ†æéŸ³é¢‘æ–‡ä»¶",
    description="""
    **æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œæ”¯æŒè¯¦ç»†çš„ä»»åŠ¡ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª**

    ## ç‰¹ç‚¹
    - ğŸ“ **å¤šæ–‡ä»¶å¹¶è¡Œå¤„ç†**
    - ğŸ¯ **è¯¦ç»†çš„ä»»åŠ¡ç®¡ç†**
    - ğŸ“Š **å®æ—¶è¿›åº¦è·Ÿè¸ª**
    - ğŸ”§ **çµæ´»çš„ä¼˜å…ˆçº§è®¾ç½®**
    - ğŸ’¾ **æ™ºèƒ½èµ„æºç®¡ç†**

    ## ä½¿ç”¨åœºæ™¯
    - **æ‰¹é‡å¤„ç†**: å¤„ç†å¤§é‡éŸ³é¢‘æ–‡ä»¶
    - **è‡ªåŠ¨åŒ–æµç¨‹**: é›†æˆåˆ°è‡ªåŠ¨åŒ–å·¥ä½œæµ
    - **ç”¨æˆ·ä¸Šä¼ **: æ”¯æŒç”¨æˆ·ä¸€æ¬¡ä¸Šä¼ å¤šä¸ªæ–‡ä»¶
    - **å®šæ—¶ä»»åŠ¡**: å®šæœŸæ‰¹é‡åˆ†æéŸ³é¢‘

    ## å·¥ä½œæµç¨‹

    ### 1. æäº¤æ‰¹é‡ä»»åŠ¡
    ```bash
    curl -X POST "http://localhost:8193/api/analyze/batch" \\
      -F "files=@song1.wav" \\
      -F "files=@song2.mp3" \\
      -F "model=harmonix-all" \\
      -F "priority=1"
    ```

    ### 2. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
    ```bash
    curl -X GET "http://localhost:8193/api/analyze/batch/{task_id}/status"
    ```

    ### 3. è·å–æ‰¹é‡ç»“æœ
    ```bash
    curl -X GET "http://localhost:8193/api/analyze/batch/{task_id}/result"
    ```

    ## æ–‡ä»¶éªŒè¯

    ### æ”¯æŒæ ¼å¼
    - **WAV**: æ¨èæ ¼å¼ï¼Œæ— å‹ç¼©ï¼Œåˆ†æç²¾åº¦æœ€é«˜
    - **MP3**: æ”¯æŒæ ¼å¼ï¼Œä½†å¯èƒ½æœ‰20-40msæ—¶å·®

    ### æ–‡ä»¶é™åˆ¶
    - **å•ä¸ªæ–‡ä»¶**: ç”±æ ¸å¿ƒåº“å’Œç³»ç»Ÿèµ„æºå†³å®š
    - **éŸ³é¢‘æ—¶é•¿**: ç”±æ ¸å¿ƒåº“å’Œç³»ç»Ÿèµ„æºå†³å®š
    - **æ‰¹é‡æ•°é‡**: å•æ¬¡æœ€å¤š50ä¸ªæ–‡ä»¶

    ### éªŒè¯ç»“æœ
    - **valid**: æ–‡ä»¶é€šè¿‡éªŒè¯
    - **invalid_format**: ä¸æ”¯æŒçš„æ ¼å¼
    - **too_large**: æ–‡ä»¶è¿‡å¤§
    - **too_long**: éŸ³é¢‘æ—¶é•¿è¶…é™

    ## ä¼˜å…ˆçº§è®¾ç½®

    | ä¼˜å…ˆçº§ | å¤„ç†é¡ºåº | é€‚ç”¨åœºæ™¯ |
    |--------|----------|----------|
    | 1 | é«˜ä¼˜å…ˆçº§ | ç´§æ€¥ä»»åŠ¡ã€VIPç”¨æˆ· |
    | 2 | æ­£å¸¸ä¼˜å…ˆçº§ | å¸¸è§„ä»»åŠ¡ï¼ˆé»˜è®¤ï¼‰ |
    | 3 | ä½ä¼˜å…ˆçº§ | åå°æ‰¹é‡ä»»åŠ¡ |

    ## ä»»åŠ¡çŠ¶æ€è¯´æ˜

    - **pending**: ä»»åŠ¡æ’é˜Ÿä¸­ï¼Œç­‰å¾…å¤„ç†
    - **processing**: æ­£åœ¨å¤„ç†æ–‡ä»¶
    - **completed**: æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ
    - **failed**: å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯
    - **cancelled**: ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ

    ## è¿›åº¦ä¿¡æ¯ç¤ºä¾‹
    ```json
    {
      "task_id": "batch_20241123_001",
      "status": "processing",
      "progress": 60.0,
      "current_file": "song3.wav",
      "completed_files": ["song1.wav", "song2.wav"],
      "failed_files": [],
      "estimated_remaining": "2-3åˆ†é’Ÿ",
      "file_count": 5,
      "completed_count": 2,
      "failed_count": 0
    }
    ```

    ## é”™è¯¯å¤„ç†

    ### å•ä¸ªæ–‡ä»¶å¤±è´¥
    - ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
    - åœ¨ç»“æœä¸­æ ‡è®°å¤±è´¥æ–‡ä»¶å’ŒåŸå› 
    - ä¸å½±å“æ•´ä¸ªæ‰¹é‡ä»»åŠ¡

    ### æ•´ä¸ªä»»åŠ¡å¤±è´¥
    - ç³»ç»Ÿçº§é”™è¯¯
    - æ‰€æœ‰æ–‡ä»¶éƒ½æ— æ³•å¤„ç†
    - ä»»åŠ¡çŠ¶æ€æ ‡è®°ä¸ºfailed

    ## æœ€ä½³å®è·µ

    ### 1. æ–‡ä»¶ç»„ç»‡
    - å»ºè®®å°†ç›¸ä¼¼ç±»å‹çš„éŸ³é¢‘æ–‡ä»¶æ”¾åœ¨åŒä¸€æ‰¹æ¬¡
    - é¿å…æ··ç”¨ä¸åŒè´¨é‡å‚æ•°çš„æ–‡ä»¶

    ### 2. èµ„æºç®¡ç†
    - æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è®¾ç½®åˆç†çš„æ‰¹æ¬¡å¤§å°
    - é«˜å³°æœŸä½¿ç”¨è¾ƒä½ä¼˜å…ˆçº§

    ### 3. ç›‘æ§å’Œå‘Šè­¦
    - å®šæœŸæ£€æŸ¥ä»»åŠ¡çŠ¶æ€
    - è®¾ç½®å¤±è´¥ç‡å‘Šè­¦é˜ˆå€¼
    - ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

    ## å®¢æˆ·ç«¯é›†æˆç¤ºä¾‹

    ### JavaScript å‰ç«¯
    ```javascript
    // æäº¤æ‰¹é‡ä»»åŠ¡
    const submitBatchTask = async (files, options = {}) => {
      const formData = new FormData();

      files.forEach(file => {
        formData.append('files', file);
      });

      formData.append('model', options.model || 'harmonix-all');
      formData.append('priority', options.priority || 2);

      const response = await fetch('/api/analyze/batch', {
        method: 'POST',
        body: formData
      });

      return await response.json();
    };

    // ç›‘æ§æ‰¹é‡ä»»åŠ¡è¿›åº¦
    const monitorBatchProgress = async (taskId, onUpdate) => {
      const checkStatus = async () => {
        const response = await fetch(`/api/analyze/batch/${taskId}/status`);
        const status = await response.json();

        onUpdate(status);

        if (status.status === 'processing') {
          setTimeout(checkStatus, 2000); // æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
        }

        return status;
      };

      return await checkStatus();
    };

    // è·å–æ‰¹é‡ç»“æœ
    const getBatchResult = async (taskId) => {
      const response = await fetch(`/api/analyze/batch/${taskId}/result`);
      return await response.json();
    };
    ```

    ## æ€§èƒ½ä¼˜åŒ–

    ### å¹¶è¡Œå¤„ç†
    - æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´å¹¶å‘æ•°é‡
    - é¿å…è¿‡åº¦å¹¶è¡Œå¯¼è‡´èµ„æºç«äº‰

    ### å†…å­˜ç®¡ç†
    - åŠæ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    - ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ

    ### ç¼“å­˜ç­–ç•¥
    - ç¼“å­˜æ¨¡å‹åŠ è½½ç»“æœ
    - é‡ç”¨é¢‘è°±å›¾è®¡ç®—ç»“æœ
    """,
    deprecated=False
)
async def submit_batch_analysis(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(
        ...,
        description="""
        ## éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨

        æ”¯æŒ1-50ä¸ªæ–‡ä»¶åŒæ—¶ä¸Šä¼ ã€‚

        ### æ–‡ä»¶è¦æ±‚
        - **æ ¼å¼**: WAV (æ¨è) æˆ– MP3
        - **å¤§å°**: æ¯ä¸ªæ–‡ä»¶ç”±æ ¸å¿ƒåº“å’Œç³»ç»Ÿèµ„æºå†³å®š
        - **æ—¶é•¿**: æ¯ä¸ªæ–‡ä»¶ç”±æ ¸å¿ƒåº“å’Œç³»ç»Ÿèµ„æºå†³å®š
        - **æ•°é‡**: å•æ¬¡æœ€å¤š50ä¸ªæ–‡ä»¶
        """
    ),
    model: ModelType = Form(
        default=ModelType.HARMONIX_ALL,
        description="""
        ## åˆ†ææ¨¡å‹

        **harmonix-all**: é›†æˆ8ä¸ªæ¨¡å‹çš„å¹³å‡ç»“æœï¼ˆæ¨èï¼Œç²¾åº¦æœ€é«˜ï¼‰
        **harmonix-fold0-7**: å•ä¸ªæŠ˜æ¨¡å‹ï¼ˆé€Ÿåº¦æ›´å¿«ï¼Œç²¾åº¦ç•¥ä½ï¼‰
        """
    ),
    priority: int = Form(
        default=2,
        description="""
        ## ä»»åŠ¡ä¼˜å…ˆçº§

        **1**: é«˜ä¼˜å…ˆçº§ï¼ˆç´§æ€¥ä»»åŠ¡ï¼‰
        **2**: æ­£å¸¸ä¼˜å…ˆçº§ï¼ˆé»˜è®¤ï¼‰
        **3**: ä½ä¼˜å…ˆçº§ï¼ˆåå°ä»»åŠ¡ï¼‰
        """
    ),
    visualize: bool = Form(
        default=False,
        description="æ˜¯å¦ä¸ºæ‰€æœ‰æ–‡ä»¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"
    ),
    sonify: bool = Form(
        default=False,
        description="æ˜¯å¦ä¸ºæ‰€æœ‰æ–‡ä»¶ç”ŸæˆéŸ³é¢‘åŒ–æ ‡æ³¨"
    ),
    include_activations: bool = Form(
        default=False,
        description="æ˜¯å¦åŒ…å«æ‰€æœ‰æ–‡ä»¶çš„åŸå§‹æ¿€æ´»æ•°æ®"
    ),
    include_embeddings: bool = Form(
        default=False,
        description="æ˜¯å¦åŒ…å«æ‰€æœ‰æ–‡ä»¶çš„åµŒå…¥å‘é‡æ•°æ®"
    ),
    overwrite: bool = Form(
        default=False,
        description="æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„åˆ†æç»“æœ"
    ),
    continue_on_error: bool = Form(
        default=True,
        description="å•ä¸ªæ–‡ä»¶å¤±è´¥æ—¶æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶"
    )
) -> BatchAnalysisResponse:
    """
    æäº¤æ‰¹é‡éŸ³é¢‘åˆ†æä»»åŠ¡

    åˆ›å»ºæ‰¹é‡å¤„ç†ä»»åŠ¡ï¼Œæ”¯æŒå¤šæ–‡ä»¶å¹¶è¡Œåˆ†æã€‚
    è¿”å›ä»»åŠ¡IDå’Œè¯¦ç»†çš„æ–‡ä»¶éªŒè¯ä¿¡æ¯ã€‚
    """
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"

    try:
        # éªŒè¯æ–‡ä»¶æ•°é‡
        if len(files) == 0:
            raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶")

        if len(files) > 50:
            raise HTTPException(status_code=400, detail="å•æ¬¡æœ€å¤šæ”¯æŒ50ä¸ªæ–‡ä»¶")

        # éªŒè¯ä¼˜å…ˆçº§
        if priority not in [1, 2, 3]:
            raise HTTPException(status_code=400, detail="priority å¿…é¡»æ˜¯ 1ã€2 æˆ– 3")

        # å¼ºåˆ¶ä½¿ç”¨CPUè®¾å¤‡ï¼Œå¿½ç•¥è¯·æ±‚ä¸­çš„deviceå‚æ•°
        device = "cpu"
        logger.info(
            "æäº¤æ‰¹é‡åˆ†æä»»åŠ¡",
            task_id=task_id,
            file_count=len(files),
            model=model.value,
            device=device,
            priority=priority
        )

        # éªŒè¯æ‰€æœ‰æ–‡ä»¶
        validated_files = []
        total_size = 0
        invalid_files = []

        for i, file in enumerate(files):
            try:
                # åŸºç¡€éªŒè¯
                is_valid, error_msg = await validate_audio_file(file)

                file_info = {
                    "filename": file.filename,
                    "original_index": i,
                    "status": "valid",
                    "error": None
                }

                if not is_valid:
                    file_info["status"] = f"invalid_{error_msg.split(':')[0].lower()}"
                    file_info["error"] = error_msg
                    invalid_files.append(file_info)
                    continue

                # è·å–æ–‡ä»¶å¤§å°ä¿¡æ¯
                content = await file.read()
                file_size = len(content)
                await file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ

                file_info["size_bytes"] = file_size
                file_info["size_mb"] = round(file_size / (1024 * 1024), 2)
                total_size += file_size

                validated_files.append(file_info)

            except Exception as e:
                file_info = {
                    "filename": file.filename,
                    "original_index": i,
                    "status": "validation_error",
                    "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}"
                }
                invalid_files.append(file_info)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ–‡ä»¶
        if len(validified_files) == 0:
            raise HTTPException(
                status_code=400,
                detail="æ²¡æœ‰æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå¤§å°"
            )

        # ä¼°ç®—å¤„ç†æ—¶é—´ï¼ˆå‡è®¾æ¯ä¸ªæ–‡ä»¶å¹³å‡å¤„ç†æ—¶é—´ï¼‰
        avg_time_per_file = 30  # CPUå¹³å‡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        estimated_seconds = len(validated_files) * avg_time_per_file
        estimated_time = f"{estimated_seconds//60}-{estimated_seconds//60 + 1}åˆ†é’Ÿ"

        # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        temp_file_paths = []
        file_ids = []

        for file_info in validated_files:
            original_file = files[file_info["original_index"]]

            # ä¿å­˜æ–‡ä»¶
            upload_path, file_id = await memory_file_handler.save_to_temp(original_file)

            # è·å–éŸ³é¢‘æ—¶é•¿ä¿¡æ¯ï¼ˆç”¨äºæ˜¾ç¤ºï¼Œä¸åšé™åˆ¶ï¼‰
            duration = await get_audio_duration(upload_path)
            if duration:
                logger.info("éŸ³é¢‘æ—¶é•¿ä¿¡æ¯", duration=duration, filename=original_file.filename)
                file_info["duration"] = duration

            temp_file_paths.append(upload_path)
            file_ids.append(file_id)
            file_info["temp_path"] = str(upload_path)
            file_info["file_id"] = file_id
            file_info["duration"] = duration

        # é‡æ–°è®¡ç®—æœ‰æ•ˆæ–‡ä»¶æ•°é‡
        valid_files = [f for f in validated_files if f["status"] == "valid"]

        if len(valid_files) == 0:
            raise HTTPException(
                status_code=400,
                detail="æ²¡æœ‰ç¬¦åˆè¦æ±‚çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆæ—¶é•¿è¶…è¿‡é™åˆ¶ï¼‰"
            )

        # åˆ›å»ºæ‰¹é‡ä»»åŠ¡è®°å½•
        batch_tasks[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "priority": priority,
            "file_count": len(valid_files),
            "valid_files": valid_files,
            "invalid_files": invalid_files,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "temp_file_paths": temp_file_paths,
            "file_ids": file_ids,
            "request": {
                "model": model,
                "device": device,
                "visualize": visualize,
                "sonify": sonify,
                "include_activations": include_activations,
                "include_embeddings": include_embeddings,
                "overwrite": overwrite,
                "continue_on_error": continue_on_error
            },
            "progress": {
                "current_file": None,
                "completed_files": [],
                "failed_files": [],
                "results": {},
                "completed_count": 0,
                "failed_count": 0
            },
            "timing": {
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "started_at": None,
                "completed_at": None
            },
            "estimated_time": estimated_time
        }

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            _process_batch_analysis_task,
            task_id
        )

        logger.info(
            "æ‰¹é‡åˆ†æä»»åŠ¡å·²åˆ›å»º",
            task_id=task_id,
            valid_files=len(valid_files),
            invalid_files=len(invalid_files),
            total_size_mb=batch_tasks[task_id]["total_size_mb"],
            estimated_time=estimated_time
        )

        return BatchAnalysisResponse(
            success=True,
            task_id=task_id,
            message=f"æ‰¹é‡åˆ†æä»»åŠ¡å·²åˆ›å»ºï¼Œå…±{len(valid_files)}ä¸ªæ–‡ä»¶",
            estimated_time=estimated_time,
            file_count=len(valid_files),
            priority=priority
        )

    except Exception as e:
        # æ¸…ç†å·²ä¿å­˜çš„ä¸´æ—¶æ–‡ä»¶
        if 'file_ids' in locals():
            for file_id in file_ids:
                await memory_file_handler.cleanup_file(file_id)

        logger.error("åˆ›å»ºæ‰¹é‡åˆ†æä»»åŠ¡å¤±è´¥", task_id=task_id, error=str(e))
        raise HTTPException(
            status_code=500,
            detail=f"åˆ›å»ºæ‰¹é‡ä»»åŠ¡å¤±è´¥: {str(e)}"
        )

async def _process_batch_analysis_task(task_id: str):
    """
    å¤„ç†æ‰¹é‡åˆ†æä»»åŠ¡ï¼ˆåå°è¿è¡Œï¼‰

    Args:
        task_id: ä»»åŠ¡ID
    """
    if task_id not in batch_tasks:
        return

    task = batch_tasks[task_id]
    temp_file_paths = task["temp_file_paths"]
    request_params = task["request"]
    progress = task["progress"]
    timing = task["timing"]

    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
        task["status"] = "processing"
        timing["started_at"] = datetime.now().isoformat()
        timing["updated_at"] = timing["started_at"]

        valid_files = task["valid_files"]
        total_files = len(valid_files)

        logger.info(
            "å¼€å§‹å¤„ç†æ‰¹é‡åˆ†æä»»åŠ¡",
            task_id=task_id,
            total_files=total_files,
            priority=task["priority"]
        )

        for i, file_info in enumerate(valid_files):
            try:
                # æ›´æ–°è¿›åº¦
                progress["current_file"] = file_info["filename"]
                progress["completed_count"] = len(progress["completed_files"])
                progress["failed_count"] = len(progress["failed_files"])
                overall_progress = (i / total_files) * 100
                task["progress"] = {**progress, "overall_progress": overall_progress}
                timing["updated_at"] = datetime.now().isoformat()

                logger.info(
                    "å¤„ç†æ‰¹é‡ä»»åŠ¡æ–‡ä»¶",
                    task_id=task_id,
                    file_index=i + 1,
                    total_files=total_files,
                    filename=file_info["filename"],
                    progress=f"{overall_progress:.1f}%"
                )

                # åˆ›å»ºå•ä¸ªæ–‡ä»¶åˆ†æè¯·æ±‚ï¼ˆå¼ºåˆ¶ä½¿ç”¨CPUï¼‰
                from ..models import AnalysisRequest, DeviceType
                single_request = AnalysisRequest(
                    model=request_params["model"],
                    device=DeviceType.CPU,  # å¼ºåˆ¶ä½¿ç”¨CPU
                    visualize=request_params["visualize"],
                    sonify=request_params["sonify"],
                    include_activations=request_params["include_activations"],
                    include_embeddings=request_params["include_embeddings"],
                    overwrite=request_params["overwrite"]
                )

                # åˆ†ææ–‡ä»¶
                result_data, file_links = await analysis_service.analyze_single_file(
                    Path(file_info["temp_path"]),
                    single_request,
                    f"{task_id}_{i}"
                )

                # ä¿å­˜ç»“æœ
                progress["results"][file_info["filename"]] = {
                    "data": result_data,
                    "files": file_links,
                    "success": True
                }
                progress["completed_files"].append(file_info["filename"])

            except Exception as e:
                logger.error(
                    "æ‰¹é‡ä»»åŠ¡æ–‡ä»¶å¤„ç†å¤±è´¥",
                    task_id=task_id,
                    filename=file_info["filename"],
                    error=str(e)
                )

                progress["results"][file_info["filename"]] = {
                    "success": False,
                    "error": str(e)
                }
                progress["failed_files"].append(file_info["filename"])

                # å¦‚æœè®¾ç½®äº†ä¸ç»§ç»­å¤„ç†é”™è¯¯ï¼Œåˆ™ä¸­æ–­
                if not request_params["continue_on_error"]:
                    logger.warning(
                        "æ‰¹é‡ä»»åŠ¡å› é”™è¯¯ä¸­æ–­",
                        task_id=task_id,
                        error=str(e)
                    )
                    break

        # ä»»åŠ¡å®Œæˆ
        task["status"] = "completed"
        timing["completed_at"] = datetime.now().isoformat()
        timing["updated_at"] = timing["completed_at"]
        progress["current_file"] = None
        progress["overall_progress"] = 100.0
        task["progress"] = progress

        logger.info(
            "æ‰¹é‡åˆ†æä»»åŠ¡å®Œæˆ",
            task_id=task_id,
            completed_count=len(progress["completed_files"]),
            failed_count=len(progress["failed_files"]),
            total_processing_time=(
                datetime.fromisoformat(timing["completed_at"]) -
                datetime.fromisoformat(timing["started_at"])
            ).total_seconds()
        )

    except Exception as e:
        logger.error(
            "æ‰¹é‡åˆ†æä»»åŠ¡å¤„ç†å¤±è´¥",
            task_id=task_id,
            error=str(e)
        )
        task["status"] = "failed"
        timing["updated_at"] = datetime.now().isoformat()

    finally:
        # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        for file_id in task["file_ids"]:
            await memory_file_handler.cleanup_file(file_id)

@router.get(
    "/analyze/batch/{task_id}/status",
    summary="æŸ¥è¯¢æ‰¹é‡åˆ†æä»»åŠ¡çŠ¶æ€",
    description="""
    æŸ¥è¯¢æ‰¹é‡åˆ†æä»»åŠ¡çš„æ‰§è¡ŒçŠ¶æ€å’Œè¯¦ç»†è¿›åº¦ä¿¡æ¯ã€‚

    ## è¿”å›ä¿¡æ¯
    - ä»»åŠ¡çŠ¶æ€ï¼ˆpending/processing/completed/failedï¼‰
    - è¯¦ç»†è¿›åº¦ï¼ˆæ–‡ä»¶çº§åˆ«ï¼‰
    - å·²å®Œæˆå’Œå¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨
    - æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
    """
)
async def get_batch_analysis_status(task_id: str):
    """
    è·å–æ‰¹é‡åˆ†æä»»åŠ¡çŠ¶æ€

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        Dict: ä»»åŠ¡çŠ¶æ€å’Œè¿›åº¦ä¿¡æ¯
    """
    if task_id not in batch_tasks:
        raise HTTPException(
            status_code=404,
            detail=f"æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
        )

    task = batch_tasks[task_id]
    progress = task["progress"]
    timing = task["timing"]

    # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
    estimated_remaining = None
    if task["status"] == "processing" and progress["completed_count"] > 0:
        elapsed_time = (
            datetime.now() -
            datetime.fromisoformat(timing["started_at"])
        ).total_seconds()

        avg_time_per_file = elapsed_time / progress["completed_count"]
        remaining_files = task["file_count"] - progress["completed_count"]
        estimated_seconds = remaining_files * avg_time_per_file

        if estimated_seconds > 0:
            estimated_remaining = f"{int(estimated_seconds // 60)}-{int((estimated_seconds // 60) + 1)}åˆ†é’Ÿ"

    return {
        "task_id": task_id,
        "status": task["status"],
        "priority": task["priority"],
        "progress": {
            "overall_progress": progress.get("overall_progress", 0),
            "current_file": progress["current_file"],
            "completed_files": progress["completed_files"],
            "failed_files": progress["failed_files"],
            "file_count": task["file_count"],
            "completed_count": progress["completed_count"],
            "failed_count": progress["failed_count"],
            "estimated_remaining": estimated_remaining
        },
        "timing": {
            "created_at": timing["created_at"],
            "started_at": timing.get("started_at"),
            "updated_at": timing["updated_at"],
            "estimated_time": task["estimated_time"]
        },
        "file_summary": {
            "valid_files": len(task["valid_files"]),
            "invalid_files": len(task["invalid_files"]),
            "total_size_mb": task["total_size_mb"]
        }
    }

@router.get(
    "/analyze/batch/{task_id}/result",
    summary="è·å–æ‰¹é‡åˆ†æç»“æœ",
    description="""
    è·å–å·²å®Œæˆçš„æ‰¹é‡åˆ†æä»»åŠ¡çš„è¯¦ç»†ç»“æœã€‚

    ## è¿”å›ä¿¡æ¯
    - å®Œæ•´çš„åˆ†æç»“æœåˆ—è¡¨
    - æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†çŠ¶æ€
    - ç”Ÿæˆçš„æ–‡ä»¶ä¸‹è½½é“¾æ¥
    - é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
)
async def get_batch_analysis_result(task_id: str):
    """
    è·å–æ‰¹é‡åˆ†æä»»åŠ¡ç»“æœ

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        Dict: æ‰¹é‡åˆ†æç»“æœ
    """
    if task_id not in batch_tasks:
        raise HTTPException(
            status_code=404,
            detail=f"æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
        )

    task = batch_tasks[task_id]
    progress = task["progress"]
    timing = task["timing"]

    response = {
        "task_id": task_id,
        "status": task["status"],
        "message": "æ‰¹é‡åˆ†æå®Œæˆ" if task["status"] == "completed" else "åˆ†æå¤±è´¥",
        "file_count": task["file_count"],
        "completed_count": progress["completed_count"],
        "failed_count": progress["failed_count"],
        "timing": timing
    }

    if task["status"] == "completed":
        # æ·»åŠ æˆåŠŸçš„ç»“æœ
        results = []
        files = {}

        for filename, result_data in progress["results"].items():
            if result_data["success"]:
                results.append(result_data["data"])
                if result_data.get("files"):
                    files[filename] = result_data["files"]

        response["success"] = True
        response["results"] = results
        response["files"] = files
        response["total_processing_time"] = (
            datetime.fromisoformat(timing["completed_at"]) -
            datetime.fromisoformat(timing["started_at"])
        ).total_seconds() if timing.get("started_at") else None

    elif task["status"] == "failed":
        response["success"] = False
        response["error"] = "æ‰¹é‡åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"

    return response

@router.delete(
    "/analyze/batch/{task_id}",
    summary="å–æ¶ˆæˆ–åˆ é™¤æ‰¹é‡åˆ†æä»»åŠ¡",
    description="""
    å–æ¶ˆæ­£åœ¨å¤„ç†çš„æ‰¹é‡ä»»åŠ¡æˆ–åˆ é™¤å·²å®Œæˆçš„ä»»åŠ¡è®°å½•ã€‚

    ## æ³¨æ„äº‹é¡¹
    - æ­£åœ¨å¤„ç†çš„ä»»åŠ¡åªèƒ½å–æ¶ˆï¼Œæ— æ³•åˆ é™¤
    - å·²å®Œæˆçš„ä»»åŠ¡å¯ä»¥åˆ é™¤
    - åˆ é™¤åæ— æ³•æ¢å¤ç»“æœ
    """
)
async def cancel_or_delete_batch_task(task_id: str):
    """
    å–æ¶ˆæˆ–åˆ é™¤æ‰¹é‡åˆ†æä»»åŠ¡

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        Dict: æ“ä½œç»“æœ
    """
    if task_id not in batch_tasks:
        raise HTTPException(
            status_code=404,
            detail=f"æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
        )

    task = batch_tasks[task_id]

    if task["status"] in ["pending", "processing"]:
        # å–æ¶ˆæ­£åœ¨å¤„ç†çš„ä»»åŠ¡
        task["status"] = "cancelled"
        task["timing"]["updated_at"] = datetime.now().isoformat()

        logger.info("æ‰¹é‡ä»»åŠ¡å·²å–æ¶ˆ", task_id=task_id)
        return {
            "success": True,
            "message": f"æ‰¹é‡ä»»åŠ¡å·²å–æ¶ˆ: {task_id}"
        }
    elif task["status"] in ["completed", "failed", "cancelled"]:
        # åˆ é™¤å·²å®Œæˆçš„ä»»åŠ¡è®°å½•
        del batch_tasks[task_id]

        logger.info("æ‰¹é‡ä»»åŠ¡è®°å½•å·²åˆ é™¤", task_id=task_id)
        return {
            "success": True,
            "message": f"æ‰¹é‡ä»»åŠ¡è®°å½•å·²åˆ é™¤: {task_id}"
        }
    else:
        raise HTTPException(
            status_code=400,
            detail=f"æ— æ³•æ“ä½œå½“å‰çŠ¶æ€çš„ä»»åŠ¡: {task['status']}"
        )

@router.get(
    "/analyze/batch/list",
    summary="è·å–æ‰¹é‡ä»»åŠ¡åˆ—è¡¨",
    description="""
    è·å–æ‰€æœ‰æ‰¹é‡ä»»åŠ¡çš„ç®€è¦ä¿¡æ¯åˆ—è¡¨ã€‚

    ## è¿”å›ä¿¡æ¯
    - ä»»åŠ¡IDåˆ—è¡¨
    - ä»»åŠ¡çŠ¶æ€å’Œè¿›åº¦
    - åˆ›å»ºå’Œæ›´æ–°æ—¶é—´
    - æ–‡ä»¶æ•°é‡ç»Ÿè®¡
    """
)
async def list_batch_tasks(
    status: Optional[str] = None,
    limit: int = 50
):
    """
    è·å–æ‰¹é‡ä»»åŠ¡åˆ—è¡¨

    Args:
        status: æŒ‰çŠ¶æ€è¿‡æ»¤ä»»åŠ¡ (pending/processing/completed/failed/cancelled)
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

    Returns:
        Dict: ä»»åŠ¡åˆ—è¡¨
    """
    try:
        tasks = []

        for task_id, task_data in batch_tasks.items():
            # çŠ¶æ€è¿‡æ»¤
            if status and task_data["status"] != status:
                continue

            tasks.append({
                "task_id": task_id,
                "status": task_data["status"],
                "priority": task_data["priority"],
                "file_count": task_data["file_count"],
                "completed_count": len(task_data["progress"]["completed_files"]),
                "failed_count": len(task_data["progress"]["failed_files"]),
                "total_size_mb": task_data["total_size_mb"],
                "created_at": task_data["timing"]["created_at"],
                "updated_at": task_data["timing"]["updated_at"],
                "estimated_time": task_data.get("estimated_time")
            })

        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        tasks.sort(key=lambda x: x["created_at"], reverse=True)

        # é™åˆ¶ç»“æœæ•°é‡
        tasks = tasks[:limit]

        return {
            "success": True,
            "total_count": len(tasks),
            "tasks": tasks
        }

    except Exception as e:
        logger.error("è·å–æ‰¹é‡ä»»åŠ¡åˆ—è¡¨å¤±è´¥", error=str(e))
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}"
        )